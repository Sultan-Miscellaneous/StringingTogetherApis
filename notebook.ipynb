{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "import matplotlib\n",
    "import pandas as pd\n",
    "from habanero import Crossref \n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from scholarly import scholarly"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# read bik dataset into pandas table, replace NaNs with 0s and get first 214 entries (all other rows are empty)\n",
    "table = pd.read_csv('./bik_dataset.tsv', sep='\\t', encoding = \"ISO-8859-1\").fillna(0)[0:214]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# add affiliation categories\n",
    "affiliation_categories = ['department', 'university', 'city', 'state', 'country']\n",
    "for cat in affiliation_categories:\n",
    "    table[cat] = pd.NaT\n",
    "table['publisher'] = pd.NaT"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "try: # try to load last res_list saved\n",
    "    with open('res_list.pickle', 'rb') as handle:\n",
    "        res_list = pickle.load(handle)\n",
    "except (OSError, IOError) as e: # if can't find the pickle file reload it from crossref api\n",
    "    cr = Crossref()\n",
    "    res_list = []\n",
    "    failed_dois = []\n",
    "    for doi in tqdm(table['DOI']):\n",
    "        try:\n",
    "            res = cr.works(ids = doi)\n",
    "            res_list.append((doi, res['message']))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            failed_dois.append(doi)\n",
    "    with open('res_list.pickle', 'wb') as handle:\n",
    "        pickle.dump(res_list, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('failed_dois.pickle', 'wb') as handle:\n",
    "        pickle.dump(failed_dois, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "#use output from citeref api to get affiliation and update table\n",
    "no_affiliation_authors = []\n",
    "for r in res_list:\n",
    "    doi, data = r\n",
    "    first_author = data['author'][0]\n",
    "    last_author = data['author'][-1]\n",
    "    target_author = last_author if (len(last_author['affiliation']) > 0) else first_author if (len(first_author['affiliation']) > 0) else None\n",
    "    if target_author is not None:\n",
    "        affiliation_breakdown = target_author['affiliation'][0]['name'].split(',')\n",
    "        affiliations = {}\n",
    "        for idx, category in enumerate(affiliation_categories):\n",
    "            try:\n",
    "                affiliations[category] = affiliation_breakdown[idx].strip()\n",
    "            except IndexError:\n",
    "                break\n",
    "        for cat, val in affiliations.items():\n",
    "            table.loc[table['DOI'] == doi, cat] = val\n",
    "    else:\n",
    "        author_name = last_author['given'] + ' ' + last_author['family']\n",
    "        no_affiliation_authors.append(author_name)\n",
    "    table.loc[table['DOI'] == doi, 'publisher'] = data['publisher']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# use google scholar api to get affiliation. Note that first result in search query may not be the right author\n",
    "# some asian names are family then first name rather than the opposite\n",
    "author_dict = {}\n",
    "for author in tqdm(no_affiliation_authors[:10]):\n",
    "    search_query = scholarly.search_author(author)\n",
    "    try:\n",
    "        first_author_result = next(search_query)\n",
    "        author_dict[author] = scholarly.fill(first_author_result)\n",
    "        print(f'FOUND SOME RESULTS FOR {author}')\n",
    "    except:\n",
    "        print(f'FAILED TO FIND {author}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 10%|█         | 1/10 [00:01<00:14,  1.60s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FAILED TO FIND Hermann Ammer\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 20%|██        | 2/10 [00:11<00:50,  6.27s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FOUND SOME RESULTS FOR Susan K. Dutcher\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 30%|███       | 3/10 [00:20<00:53,  7.65s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FOUND SOME RESULTS FOR Lucille London\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 40%|████      | 4/10 [00:21<00:30,  5.10s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FAILED TO FIND Min-Jean Yin\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 50%|█████     | 5/10 [00:27<00:27,  5.49s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FOUND SOME RESULTS FOR Huangui Xiong\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 60%|██████    | 6/10 [01:25<01:32, 23.09s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FOUND SOME RESULTS FOR Yue Yang\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 70%|███████   | 7/10 [01:32<00:53, 17.85s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FOUND SOME RESULTS FOR Renato Morona\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 80%|████████  | 8/10 [01:53<00:37, 18.96s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FOUND SOME RESULTS FOR Francesco P. Schena\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 90%|█████████ | 9/10 [02:03<00:16, 16.26s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FOUND SOME RESULTS FOR Zhiwei Wang\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 10/10 [02:04<00:00, 12.50s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FAILED TO FIND Ruth Gabizon\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('.venv': poetry)"
  },
  "interpreter": {
   "hash": "a6208f7689ee996cb4b0a79af2d27023ae63a0637228143b308cc3bd1db27b34"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}